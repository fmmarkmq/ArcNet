{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN: 0.98\n",
    "ABC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/wzong/anaconda3/envs/atd2022/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.tools import dotdict\n",
    "from driver.driver import ABC_Driver\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_args = dotdict()\n",
    "mnist_args.train = dotdict()\n",
    "mnist_args.predict = dotdict()\n",
    "\n",
    "mnist_args.name = 'mnist'\n",
    "mnist_args.train.batch_size = 100\n",
    "mnist_args.predict.batch_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dotdict()\n",
    "args.data = mnist_args\n",
    "\n",
    "args.train_epochs= 50\n",
    "args.lr = 0.00008\n",
    "args.criterion = 'CE'\n",
    "args.use_gpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:None\n"
     ]
    }
   ],
   "source": [
    "driver = ABC_Driver(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('conv1.weight',\n",
       "              tensor([[[[-0.1961,  0.2542, -0.1560],\n",
       "                        [ 0.2752,  0.2147,  0.0503],\n",
       "                        [ 0.0319, -0.0352,  0.1630]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1888, -0.3261, -0.1802],\n",
       "                        [ 0.2569, -0.1947,  0.0623],\n",
       "                        [-0.2874,  0.0351, -0.0757]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2095, -0.1343,  0.0191],\n",
       "                        [ 0.3052,  0.1760, -0.0755],\n",
       "                        [-0.0155, -0.0199,  0.1003]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0317,  0.2261, -0.0905],\n",
       "                        [ 0.0794,  0.1002,  0.2647],\n",
       "                        [ 0.2898, -0.2623,  0.2637]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0014,  0.3144, -0.2402],\n",
       "                        [ 0.1984,  0.1357, -0.2085],\n",
       "                        [-0.2271,  0.0937,  0.0033]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1536, -0.2882,  0.1562],\n",
       "                        [-0.0567,  0.0981,  0.1704],\n",
       "                        [-0.2322, -0.0850,  0.0268]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1280,  0.2282,  0.0729],\n",
       "                        [ 0.1692, -0.0388,  0.0671],\n",
       "                        [-0.1826, -0.1534,  0.0433]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1647,  0.3064, -0.2449],\n",
       "                        [ 0.0801,  0.3143, -0.1586],\n",
       "                        [-0.1061,  0.0910, -0.0772]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.3234, -0.1139, -0.0104],\n",
       "                        [-0.2215,  0.0381,  0.1319],\n",
       "                        [-0.2468,  0.1005,  0.1489]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.3042, -0.1249,  0.0051],\n",
       "                        [-0.0223, -0.0707,  0.2053],\n",
       "                        [ 0.3103,  0.1754,  0.2680]]]], device='cuda:1')),\n",
       "             ('conv1.bias',\n",
       "              tensor([ 0.0898,  0.2418,  0.2266,  0.1075,  0.3167, -0.1741, -0.1670,  0.1576,\n",
       "                       0.1147,  0.2377], device='cuda:1')),\n",
       "             ('fc1.weight',\n",
       "              tensor([[-0.0026, -0.0077,  0.0046,  ...,  0.0064, -0.0107, -0.0063],\n",
       "                      [-0.0029,  0.0009,  0.0012,  ..., -0.0111, -0.0002, -0.0030],\n",
       "                      [-0.0074, -0.0034,  0.0006,  ...,  0.0085,  0.0117, -0.0069],\n",
       "                      ...,\n",
       "                      [-0.0086, -0.0017,  0.0004,  ...,  0.0019,  0.0018, -0.0111],\n",
       "                      [-0.0026, -0.0080, -0.0046,  ...,  0.0038,  0.0110, -0.0031],\n",
       "                      [ 0.0028, -0.0082, -0.0105,  ..., -0.0058,  0.0066, -0.0043]],\n",
       "                     device='cuda:1')),\n",
       "             ('fc1.bias',\n",
       "              tensor([ 0.0082, -0.0002, -0.0057, -0.0109, -0.0038,  0.0093, -0.0114,  0.0056,\n",
       "                      -0.0088, -0.0043], device='cuda:1'))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train_loss: 1.714283206264178\n",
      "epoch: 1, train_loss: 1.5675426934162775\n",
      "epoch: 2, train_loss: 1.5488774698972703\n",
      "epoch: 3, train_loss: 1.5391576705376306\n",
      "epoch: 4, train_loss: 1.53229785323143\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'Image'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/wzong/Attention_based_CNN/Will_Exp/driver/driver.py:37\u001b[0m, in \u001b[0;36mABC_Driver.train\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtrain_epochs):\n\u001b[1;32m     36\u001b[0m     train_loss\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, (inputs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     38\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     39\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/scratch/wzong/anaconda3/envs/atd2022/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/scratch/wzong/anaconda3/envs/atd2022/lib/python3.9/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/scratch/wzong/anaconda3/envs/atd2022/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/scratch/wzong/anaconda3/envs/atd2022/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/scratch/wzong/anaconda3/envs/atd2022/lib/python3.9/site-packages/torchvision/datasets/mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m/scratch/wzong/anaconda3/envs/atd2022/lib/python3.9/site-packages/torchvision/transforms/transforms.py:94\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 94\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/scratch/wzong/anaconda3/envs/atd2022/lib/python3.9/site-packages/torchvision/transforms/transforms.py:134\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/wzong/anaconda3/envs/atd2022/lib/python3.9/site-packages/torchvision/transforms/functional.py:164\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[1;32m    163\u001b[0m mode_to_nptype \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint32, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint16, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mfloat32}\n\u001b[0;32m--> 164\u001b[0m img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_to_nptype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    167\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'Image'"
     ]
    }
   ],
   "source": [
    "driver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9768"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('ABC_2D.weights',\n",
       "              tensor([[[ 0.4218,  0.3002,  0.6538,  ...,  1.3585, -0.4829, -0.2831],\n",
       "                       [ 0.0428,  0.9148, -1.5040,  ..., -0.5533,  0.0680, -0.3943],\n",
       "                       [ 0.9771,  1.2894, -0.1461,  ..., -0.3790, -1.6897,  0.7358],\n",
       "                       [ 0.4761,  0.1135, -0.1265,  ...,  1.1909, -1.4262,  0.4537],\n",
       "                       [ 0.1916, -1.2229,  1.7847,  ...,  1.7269, -1.2191,  1.1335],\n",
       "                       [ 0.3579, -0.3335, -1.0616,  ...,  1.6197, -0.6532,  0.1870]],\n",
       "              \n",
       "                      [[ 0.0945,  0.1648, -2.2135,  ..., -1.6361, -0.3742, -0.4358],\n",
       "                       [ 0.9143,  1.0498, -0.4036,  ..., -0.4032, -0.2414, -0.0034],\n",
       "                       [-1.5146, -1.2316,  0.5254,  ..., -0.5536,  0.1129,  0.9378],\n",
       "                       [-1.7448,  0.4697,  0.0416,  ..., -1.1745, -2.2198, -0.0761],\n",
       "                       [-0.7729,  0.1407,  0.8280,  ...,  0.9302, -0.1723,  0.0836],\n",
       "                       [-1.2330,  0.5058,  0.7044,  ..., -0.2659,  0.1624, -0.4986]],\n",
       "              \n",
       "                      [[ 0.1080, -0.5895, -0.3810,  ...,  0.8313,  0.1284,  0.3555],\n",
       "                       [ 0.4714, -0.8463,  0.4403,  ...,  0.5580,  0.4728, -0.2824],\n",
       "                       [-1.2640, -1.3220,  1.3988,  ..., -0.0095, -1.0901,  0.6771],\n",
       "                       [ 1.8728,  1.5274,  0.4011,  ...,  1.2371, -0.7795, -0.4907],\n",
       "                       [-1.9693, -0.7685,  0.8209,  ..., -0.0520, -0.8773, -0.2264],\n",
       "                       [ 0.0196,  0.6624, -0.4805,  ...,  0.4252, -0.1237,  0.3954]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[-0.3747, -0.7132,  0.0086,  ...,  1.0840,  0.5396, -0.6496],\n",
       "                       [-0.8560,  0.0921, -0.3129,  ..., -0.2137, -0.3130,  0.7319],\n",
       "                       [ 1.7554, -0.7883,  0.5043,  ...,  0.0553,  1.3314, -0.5764],\n",
       "                       [ 1.3324,  1.5591,  0.0697,  ...,  0.3373,  1.0092, -0.8216],\n",
       "                       [ 0.5323,  2.3619, -0.9251,  ...,  0.5966,  1.5801,  0.2740],\n",
       "                       [ 1.2015, -1.1651, -0.4111,  ...,  0.2121, -1.2249, -0.1097]],\n",
       "              \n",
       "                      [[-0.6149,  0.3176, -0.9192,  ...,  1.7832,  0.8082, -1.7066],\n",
       "                       [ 0.2386, -0.0150,  0.5302,  ...,  0.8647,  0.3662,  0.3853],\n",
       "                       [ 1.9447, -0.5724, -0.2705,  ...,  0.7067,  0.0998, -0.1839],\n",
       "                       [-0.2772,  0.4259, -1.1021,  ..., -0.5470, -0.7948,  0.3693],\n",
       "                       [-0.1516, -1.6972, -1.1664,  ..., -0.1032,  0.6890,  1.0075],\n",
       "                       [ 1.6391, -0.7597, -0.8887,  ..., -1.4804, -0.0436, -2.1692]],\n",
       "              \n",
       "                      [[-0.8631,  0.3949,  1.1831,  ..., -0.3158,  1.7204,  0.1305],\n",
       "                       [-0.5914, -1.2210,  0.6190,  ...,  0.2892, -0.8127,  0.6196],\n",
       "                       [-1.3895,  1.2092, -0.0261,  ...,  0.5770, -0.9260, -0.7226],\n",
       "                       [ 1.1547,  2.0994, -0.1197,  ..., -0.7882,  0.0292,  1.1024],\n",
       "                       [-0.2261,  0.2203,  0.5958,  ..., -0.1516, -1.5572, -0.9961],\n",
       "                       [ 0.5787,  1.7214,  0.4792,  ..., -0.7516, -0.1280, -0.4396]]],\n",
       "                     device='cuda:1')),\n",
       "             ('fc1.weight',\n",
       "              tensor([[-0.0073,  0.0130, -0.0030,  ..., -0.0144,  0.0074, -0.0116],\n",
       "                      [ 0.0072,  0.0036, -0.0139,  ...,  0.0023,  0.0055,  0.0014],\n",
       "                      [ 0.0033, -0.0080,  0.0021,  ..., -0.0010,  0.0077,  0.0039],\n",
       "                      ...,\n",
       "                      [-0.0079,  0.0016,  0.0060,  ...,  0.0059, -0.0071,  0.0075],\n",
       "                      [ 0.0010, -0.0093,  0.0125,  ...,  0.0070, -0.0053, -0.0067],\n",
       "                      [ 0.0064,  0.0087,  0.0120,  ..., -0.0088, -0.0063,  0.0032]],\n",
       "                     device='cuda:1')),\n",
       "             ('fc1.bias',\n",
       "              tensor([-0.0054,  0.0111,  0.0137, -0.0029,  0.0048,  0.0110, -0.0017, -0.0057,\n",
       "                       0.0169,  0.0153], device='cuda:1'))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.MNIST('../../data/ABC/mnist', train=False, download=True,\n",
    "                                                  transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                                            torchvision.transforms.Normalize((0.1307,), (0.3081,))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: ../../data/ABC/mnist\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "           )"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[[1,2],[3,4]],[[1,2],[3,4]],[[1,2],[3,4]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7, 10],\n",
       "         [15, 22]],\n",
       "\n",
       "        [[ 7, 10],\n",
       "         [15, 22]],\n",
       "\n",
       "        [[ 7, 10],\n",
       "         [15, 22]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(a,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_args = dotdict()\n",
    "mnist_args.train = dotdict()\n",
    "mnist_args.predict = dotdict()\n",
    "\n",
    "mnist_args.name = 'mnist'\n",
    "mnist_args.train.batch_size = 100\n",
    "mnist_args.predict.batch_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train_loss: 1.8092176419496537\n",
      "epoch: 1, train_loss: 1.5854661240180334\n",
      "epoch: 2, train_loss: 1.5594474685192108\n",
      "epoch: 3, train_loss: 1.547036578655243\n",
      "epoch: 4, train_loss: 1.5385117679834366\n",
      "epoch: 5, train_loss: 1.532321839928627\n",
      "epoch: 6, train_loss: 1.527201450864474\n",
      "epoch: 7, train_loss: 1.5227444046735763\n",
      "epoch: 8, train_loss: 1.5188205428918202\n",
      "epoch: 9, train_loss: 1.5152481007575989\n",
      "epoch: 10, train_loss: 1.5122010159492492\n",
      "epoch: 11, train_loss: 1.5093879654010136\n",
      "epoch: 12, train_loss: 1.5068205038706461\n",
      "epoch: 13, train_loss: 1.504511877099673\n",
      "epoch: 14, train_loss: 1.5022801474730174\n",
      "epoch: 15, train_loss: 1.5006628227233887\n",
      "epoch: 16, train_loss: 1.4987579685449601\n",
      "epoch: 17, train_loss: 1.4970314226547876\n",
      "epoch: 18, train_loss: 1.4954067504405975\n",
      "epoch: 19, train_loss: 1.4939528038104375\n",
      "epoch: 20, train_loss: 1.492574091553688\n",
      "epoch: 21, train_loss: 1.491297993262609\n",
      "epoch: 22, train_loss: 1.4901642004648845\n",
      "epoch: 23, train_loss: 1.488807551264763\n",
      "epoch: 24, train_loss: 1.4878206253051758\n",
      "epoch: 25, train_loss: 1.4869583674271902\n",
      "epoch: 26, train_loss: 1.4859451140960058\n",
      "epoch: 27, train_loss: 1.4851976631085078\n",
      "epoch: 28, train_loss: 1.4844763058423995\n",
      "epoch: 29, train_loss: 1.4837394764026006\n",
      "epoch: 30, train_loss: 1.482983751296997\n",
      "epoch: 31, train_loss: 1.4824039614200593\n",
      "epoch: 32, train_loss: 1.4817013945182165\n",
      "epoch: 33, train_loss: 1.481126711765925\n",
      "epoch: 34, train_loss: 1.480618494351705\n",
      "epoch: 35, train_loss: 1.4799588799476624\n",
      "epoch: 36, train_loss: 1.4794500982761383\n",
      "epoch: 37, train_loss: 1.4789673934380214\n",
      "epoch: 38, train_loss: 1.4784366575876873\n",
      "epoch: 39, train_loss: 1.4780972613890966\n",
      "epoch: 40, train_loss: 1.4776452640692392\n",
      "epoch: 41, train_loss: 1.4772707253694535\n",
      "epoch: 42, train_loss: 1.4768284138043721\n",
      "epoch: 43, train_loss: 1.4764463291565577\n",
      "epoch: 44, train_loss: 1.4761250774065653\n",
      "epoch: 45, train_loss: 1.475843171477318\n",
      "epoch: 46, train_loss: 1.4754657552639643\n",
      "epoch: 47, train_loss: 1.475182980298996\n",
      "epoch: 48, train_loss: 1.4748984893163046\n",
      "epoch: 49, train_loss: 1.4746273775895438\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<driver.driver.ABC_Driver at 0x7f0540750340>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9782"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
